
import time
import re
import os
import datetime
# from selenium import webdriver # æ›¿æ¢ä¸ºundetected_chromedriver
import undetected_chromedriver as uc
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException

class SimpleBrowserCrawler:
    """ç®€åŒ–çš„æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®çˆ¬è™«"""
    
    def __init__(self, headless=True):
        self.driver = None
        self.headless = headless
        self.setup_driver()
    
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨"""
        try:
            # Chromeé€‰é¡¹é…ç½®
            chrome_options = Options()
            
            # åŸºæœ¬è®¾ç½®
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            
            # æ— å¤´æ¨¡å¼è®¾ç½®
            if self.headless:
                chrome_options.add_argument('--headless')
            
            # undetected_chromedriver å·²ç»å¤„ç†äº†å¤§éƒ¨åˆ†åæ£€æµ‹ï¼Œè¿™é‡Œå¯ä»¥ç®€åŒ–
            # chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            # chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            # chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # ç”¨æˆ·ä»£ç†è®¾ç½® (undetected_chromedriver é»˜è®¤ä¼šä½¿ç”¨éšæœºUAï¼Œè¿™é‡Œå¯ä»¥ä¿ç•™æˆ–ç§»é™¤)
            chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            # çª—å£è®¾ç½®
            chrome_options.add_argument('--window-size=1920,1080')
            
            # è¯­è¨€è®¾ç½®
            chrome_options.add_argument('--lang=zh-CN')
            
            # åˆ›å»ºWebDriverï¼Œä½¿ç”¨undetected_chromedriver
            self.driver = uc.Chrome(options=chrome_options)
            
            # undetected_chromedriver å·²ç»å¤„ç†äº†webdriverå±æ€§ï¼Œè¿™é‡Œå¯ä»¥ç§»é™¤
            # self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… Chromeæµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…Chromeæµè§ˆå™¨å’ŒChromeDriverï¼Œå¹¶å°è¯•å®‰è£…undetected-chromedriveråº“")
            self.driver = None
    
    def wait_for_page_load(self, timeout=30):
        """ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ"""
        try:
            WebDriverWait(self.driver, timeout).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            return True
        except TimeoutException:
            print("âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶")
            return False
    
    def get_page_content(self, url, wait_time=5):
        """è·å–é¡µé¢å†…å®¹"""
        if not self.driver:
            print("âŒ æµè§ˆå™¨é©±åŠ¨æœªåˆå§‹åŒ–")
            return None
        
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
            
            # è®¿é—®é¡µé¢
            self.driver.get(url)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            if not self.wait_for_page_load():
                print("âš ï¸ é¡µé¢åŠ è½½å¯èƒ½ä¸å®Œæ•´")
            
            # ç­‰å¾…ä¸€ä¸‹è®©JavaScriptæ‰§è¡Œ
            time.sleep(wait_time)
            
            # æ»šåŠ¨é¡µé¢ä»¥è§¦å‘åŠ¨æ€å†…å®¹
            print("ğŸ“œ æ»šåŠ¨é¡µé¢...")
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            # è·å–é¡µé¢æºç 
            page_source = self.driver.page_source
            
            print(f"âœ… é¡µé¢è·å–æˆåŠŸ")
            print(f"  æºç é•¿åº¦: {len(page_source)} å­—ç¬¦")
            
            return page_source
            
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥: {e}")
            return None
    
    def save_to_temp_file(self, content, filename="temp_page.html"):
        """ä¿å­˜å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶"""
        if not content:
            print("âŒ æ²¡æœ‰å†…å®¹å¯ä¿å­˜")
            return False
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            
            print(f"âœ… é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ {filename}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å†…å®¹å¤±è´¥: {e}")
            return False
    
    def close(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.driver:
            self.driver.quit()
            print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")

def extract_recent_links(html_file, days=3):
    """ä»HTMLæ–‡ä»¶ä¸­æå–æœ€è¿‘å‡ å¤©çš„é“¾æ¥"""
    try:
        # è¯»å–HTMLæ–‡ä»¶å†…å®¹
        with open(html_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # è·å–å½“å‰æ—¥æœŸ
        today = datetime.datetime.now()
        
        # æå–æ—¥æœŸå’Œé“¾æ¥
        links = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ—¥æœŸå’Œé“¾æ¥
        pattern = r'<span class="layui-badge layui-bg-cyan x">(\d{4}/\d{1,2}/\d{1,2})</span>[\s\S]*?<div class="index-post-img-small"><a href="([^"]+)"'
        matches = re.findall(pattern, content)
        
        for date_str, link in matches:
            # è§£ææ—¥æœŸ
            try:
                date_parts = date_str.split('/')
                post_date = datetime.datetime(int(date_parts[0]), int(date_parts[1]), int(date_parts[2]))
                
                # è®¡ç®—æ—¥æœŸå·®
                delta = today - post_date
                
                # å¦‚æœæ˜¯æœ€è¿‘å‡ å¤©çš„é“¾æ¥ï¼Œåˆ™æ·»åŠ åˆ°ç»“æœä¸­
                if delta.days < days:
                    links.append(link)
                    print(f"æ‰¾åˆ°æœ€è¿‘{days}å¤©å†…çš„é“¾æ¥: {link} (å‘å¸ƒæ—¥æœŸ: {date_str})")
            except Exception as e:
                print(f"è§£ææ—¥æœŸå¤±è´¥: {date_str}, é”™è¯¯: {e}")
                continue
        
        return links
    
    except Exception as e:
        print(f"æå–é“¾æ¥å¤±è´¥: {e}")
        return []

def save_links_to_file(links, filename="url.txt"):
    """ä¿å­˜é“¾æ¥åˆ°æ–‡ä»¶"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            for link in links:
                f.write(f"{link}\n")
        
        print(f"âœ… å·²å°†{len(links)}ä¸ªé“¾æ¥ä¿å­˜åˆ° {filename}")
        return True
    
    except Exception as e:
        print(f"âŒ ä¿å­˜é“¾æ¥å¤±è´¥: {e}")
        return False

def extract_v2ray_links(html_content):
    """ä»HTMLå†…å®¹ä¸­æå–V2rayè®¢é˜…åœ°å€"""
    v2ray_links = []
    # åŒ¹é… "V2ray è®¢é˜…åœ°å€" æ–‡æœ¬åçš„ç¬¬ä¸€ä¸ª URLï¼Œä¾‹å¦‚ï¼šhttps://www.85la.com/wp-content/uploads/2025/08/2025080866182eCpoU.txt
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ "V2ray è®¢é˜…åœ°å€" æ˜¯ä¸€ä¸ªæ–‡æœ¬æ ‡ç­¾ï¼Œåé¢ç´§è·Ÿç€ä¸€ä¸ªé“¾æ¥
    # å¦‚æœå®é™…HTMLç»“æ„ä¸åŒï¼Œå¯èƒ½éœ€è¦è°ƒæ•´æ­£åˆ™è¡¨è¾¾å¼
    pattern = r'V2ray è®¢é˜…åœ°å€.*?href="(https?://[^\s"]+\.txt)"'
    match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
    if match:
        link = match.group(1)
        v2ray_links.append(link)
        print(f"æ‰¾åˆ°V2rayè®¢é˜…é“¾æ¥: {link}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°V2rayè®¢é˜…é“¾æ¥")
    return v2ray_links
# Falseä¿å­˜ä¸´æ—¶htmlæ–‡ä»¶ True ä¸ä¿å­˜htmlæ–‡ä»¶
def process_links_from_file(links_file, crawler, delete_temp_files=True):

    """ä»url.txtæ–‡ä»¶ä¸­è¯»å–é“¾æ¥å¹¶é€ä¸ªè®¿é—®"""
    try:
        with open(links_file, 'r', encoding='utf-8') as f:
            links = [line.strip() for line in f if line.strip()]
        
        print(f"\nğŸ” å¼€å§‹å¤„ç† {len(links)} ä¸ªé“¾æ¥")
        
        # å®šä¹‰ä¿å­˜V2rayé“¾æ¥çš„æ–‡ä»¶å
        v2ray_output_file = "links.txt"

        for i, link in enumerate(links, 1):
            print(f"\nğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(links)} ä¸ªé“¾æ¥: {link}")
            
            # è·å–é¡µé¢å†…å®¹
            page_content = crawler.get_page_content(link)
            
            if page_content:
                # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                temp_file = f"temp_page_{i}.html"
                if crawler.save_to_temp_file(page_content, temp_file):
                    print(f"âœ… é¡µé¢å†…å®¹å·²ä¿å­˜åˆ° {temp_file}")
                    
                    # ä»å½“å‰é¡µé¢å†…å®¹ä¸­æå–V2rayé“¾æ¥
                    extracted_v2ray_links = extract_v2ray_links(page_content)
                    if extracted_v2ray_links:
                        with open(v2ray_output_file, 'a', encoding='utf-8') as f_v2ray:
                            for v2ray_link in extracted_v2ray_links:
                                f_v2ray.write(f"{v2ray_link}\n")
                        print(f"âœ… å·²å°† {len(extracted_v2ray_links)} ä¸ªV2rayé“¾æ¥è¿½åŠ åˆ° {v2ray_output_file}")
                    else:
                        print(f"âš ï¸ æœªåœ¨ {temp_file} ä¸­æ‰¾åˆ°V2rayè®¢é˜…é“¾æ¥")

                    # æ ¹æ®delete_temp_fileså‚æ•°å†³å®šæ˜¯å¦åˆ é™¤ä¸´æ—¶HTMLæ–‡ä»¶
                    if delete_temp_files:
                        try:
                            os.remove(temp_file)
                            print(f"âœ… ä¸´æ—¶æ–‡ä»¶ {temp_file} å·²åˆ é™¤")
                        except Exception as e:
                            print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")

                else:
                    print(f"âŒ ä¿å­˜é¡µé¢å†…å®¹åˆ° {temp_file} å¤±è´¥")
            else:
                print(f"âŒ æ— æ³•è·å– {link} çš„é¡µé¢å†…å®¹")
        
        return True
    
    except Exception as e:
        print(f"âŒ å¤„ç†é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
        return False

def main():
    print("ğŸš€ 85LAç½‘ç«™é“¾æ¥æå–å·¥å…·")
    print("=" * 60)
    
    url = "https://www.85la.com/"
    temp_file = "temp_page.html"
    links_file = "url.txt" # å­˜å‚¨ä»ä¸»é¡µæå–çš„é“¾æ¥
    v2ray_output_file = "links.txt" # å­˜å‚¨æœ€ç»ˆæå–çš„V2rayè®¢é˜…é“¾æ¥
    days = 3
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹ï¼ˆä½¿ç”¨æ— å¤´æ¨¡å¼ï¼‰
    crawler = SimpleBrowserCrawler(headless=True)
    
    if not crawler.driver:
        print("âŒ æ— æ³•å¯åŠ¨æµè§ˆå™¨ï¼Œè¯·æ£€æŸ¥Chromeå’ŒChromeDriverå®‰è£…")
        return
    
    # è®¾ç½®ä¸ºTrueè¡¨ç¤ºåˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Œè®¾ç½®ä¸ºFalseè¡¨ç¤ºä¿ç•™ä¸´æ—¶æ–‡ä»¶
    should_delete_temp_html = False # è°ƒè¯•æ—¶è®¾ç½®ä¸ºFalseï¼Œå‘å¸ƒæ—¶è®¾ç½®ä¸ºTrue

    try:
        # è·å–é¡µé¢å†…å®¹
        page_content = crawler.get_page_content(url)
        
        if page_content:
            # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
            if crawler.save_to_temp_file(page_content, temp_file):
                # æå–æœ€è¿‘å‡ å¤©çš„é“¾æ¥
                recent_links = extract_recent_links(temp_file, days)
                
                if recent_links:
                    # ä¿å­˜é“¾æ¥åˆ°url.txtæ–‡ä»¶
                    save_links_to_file(recent_links, links_file)
                    
                    # å¤„ç†url.txtä¸­çš„é“¾æ¥ï¼Œå¹¶æå–V2rayé“¾æ¥åˆ°links.txt
                    process_links_from_file(links_file, crawler, should_delete_temp_html)
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°æœ€è¿‘{days}å¤©å†…çš„é“¾æ¥")
                
                # åˆ é™¤ä¸»é¡µä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(temp_file)
                    print(f"âœ… ä¸´æ—¶æ–‡ä»¶ {temp_file} å·²åˆ é™¤")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            else:
                print("âŒ ä¿å­˜é¡µé¢å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶å¤±è´¥")
        else:
            print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
    
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    
    finally:
        # å…³é—­æµè§ˆå™¨
        crawler.close()
        # åˆ é™¤url.txtæ–‡ä»¶
        try:
            if os.path.exists(links_file):
                os.remove(links_file)
                print(f"âœ… ä¸´æ—¶æ–‡ä»¶ {links_file} å·²åˆ é™¤")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {links_file} å¤±è´¥: {e}")
        
        # ä¸åˆ é™¤links.txtï¼Œå› ä¸ºè¿™æ˜¯æœ€ç»ˆç»“æœæ–‡ä»¶
        # try:
        #     if os.path.exists(v2ray_output_file):
        #         os.remove(v2ray_output_file)
        #         print(f"âœ… ä¸´æ—¶æ–‡ä»¶ {v2ray_output_file} å·²åˆ é™¤")
        # except Exception as e:
        #     print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {v2ray_output_file} å¤±è´¥: {e}")

    print("\n" + "=" * 60)
    print("é“¾æ¥æå–å’Œå¤„ç†å®Œæˆ")

if __name__ == "__main__":
    main()

