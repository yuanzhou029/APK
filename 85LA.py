import time
import re
import os
import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException

class SimpleBrowserCrawler:
    """ç®€åŒ–çš„æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®çˆ¬è™«"""
    
    def __init__(self, headless=True):
        self.driver = None
        self.headless = headless
        self.setup_driver()
    
    def setup_driver(self):
        """è®¾ç½®Chromeæµè§ˆå™¨é©±åŠ¨ï¼Œå›ºå®šç‰ˆæœ¬åŒ¹é…"""
        try:
            chrome_options = Options()
            
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            if self.headless:
                chrome_options.add_argument('--headless')
            
            chrome_options.add_argument(
                '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.7204.183 Safari/537.36')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--lang=zh-CN')
            
            # æŒ‡å®šchromedriverè·¯å¾„ï¼Œç¡®ä¿å’Œä½ workflowé‡Œå®‰è£…çš„é©±åŠ¨ä¸€è‡´
            driver_path = '/usr/local/bin/chromedriver'
            
            self.driver = webdriver.Chrome(executable_path=driver_path, options=chrome_options)
            
            print("âœ… Chromeæµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            print(f"âŒ æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„Chromeå’ŒChromeDriver")
            self.driver = None
    
    def wait_for_page_load(self, timeout=30):
        try:
            WebDriverWait(self.driver, timeout).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            return True
        except TimeoutException:
            print("âš ï¸ é¡µé¢åŠ è½½è¶…æ—¶")
            return False
    
    def get_page_content(self, url, wait_time=5):
        if not self.driver:
            print("âŒ æµè§ˆå™¨é©±åŠ¨æœªåˆå§‹åŒ–")
            return None
        try:
            print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
            self.driver.get(url)
            if not self.wait_for_page_load():
                print("âš ï¸ é¡µé¢åŠ è½½å¯èƒ½ä¸å®Œæ•´")
            time.sleep(wait_time)
            print("ğŸ“œ æ»šåŠ¨é¡µé¢...")
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            page_source = self.driver.page_source
            print(f"âœ… é¡µé¢è·å–æˆåŠŸï¼Œé•¿åº¦: {len(page_source)} å­—ç¬¦")
            return page_source
        except Exception as e:
            print(f"âŒ è·å–é¡µé¢å†…å®¹å¤±è´¥: {e}")
            return None
    
    def save_to_temp_file(self, content, filename="temp_page.html"):
        if not content:
            print("âŒ æ²¡æœ‰å†…å®¹å¯ä¿å­˜")
            return False
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ… é¡µé¢å†…å®¹å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ {filename}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜å†…å®¹å¤±è´¥: {e}")
            return False
    
    def close(self):
        if self.driver:
            self.driver.quit()
            print("ğŸ”’ æµè§ˆå™¨å·²å…³é—­")

def extract_recent_links(html_file, days=3):
    try:
        with open(html_file, 'r', encoding='utf-8') as f:
            content = f.read()
        today = datetime.datetime.now()
        links = []
        pattern = r'<span class="layui-badge layui-bg-cyan x">(\d{4}/\d{1,2}/\d{1,2})</span>[\s\S]*?<div class="index-post-img-small"><a href="([^"]+)"'
        matches = re.findall(pattern, content)
        for date_str, link in matches:
            try:
                date_parts = date_str.split('/')
                post_date = datetime.datetime(int(date_parts[0]), int(date_parts[1]), int(date_parts[2]))
                delta = today - post_date
                if delta.days < days:
                    links.append(link)
                    print(f"æ‰¾åˆ°æœ€è¿‘{days}å¤©å†…çš„é“¾æ¥: {link} (å‘å¸ƒæ—¥æœŸ: {date_str})")
            except Exception as e:
                print(f"è§£ææ—¥æœŸå¤±è´¥: {date_str}, é”™è¯¯: {e}")
                continue
        return links
    except Exception as e:
        print(f"æå–é“¾æ¥å¤±è´¥: {e}")
        return []

def save_links_to_file(links, filename="url.txt"):
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            for link in links:
                f.write(f"{link}\n")
        print(f"âœ… å·²å°†{len(links)}ä¸ªé“¾æ¥ä¿å­˜åˆ° {filename}")
        return True
    except Exception as e:
        print(f"âŒ ä¿å­˜é“¾æ¥å¤±è´¥: {e}")
        return False

def extract_v2ray_links(html_content):
    v2ray_links = []
    pattern = r'V2ray è®¢é˜…åœ°å€.*?href="(https?://[^\s"]+\.txt)"'
    match = re.search(pattern, html_content, re.IGNORECASE | re.DOTALL)
    if match:
        link = match.group(1)
        v2ray_links.append(link)
        print(f"æ‰¾åˆ°V2rayè®¢é˜…é“¾æ¥: {link}")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°V2rayè®¢é˜…é“¾æ¥")
    return v2ray_links

def process_links_from_file(links_file, crawler, delete_temp_files=True):
    try:
        with open(links_file, 'r', encoding='utf-8') as f:
            links = [line.strip() for line in f if line.strip()]
        print(f"\nğŸ” å¼€å§‹å¤„ç† {len(links)} ä¸ªé“¾æ¥")
        v2ray_output_file = "links.txt"
        for i, link in enumerate(links, 1):
            print(f"\nğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {i}/{len(links)} ä¸ªé“¾æ¥: {link}")
            page_content = crawler.get_page_content(link)
            if page_content:
                temp_file = f"temp_page_{i}.html"
                if crawler.save_to_temp_file(page_content, temp_file):
                    extracted_v2ray_links = extract_v2ray_links(page_content)
                    if extracted_v2ray_links:
                        with open(v2ray_output_file, 'a', encoding='utf-8') as f_v2ray:
                            for v2ray_link in extracted_v2ray_links:
                                f_v2ray.write(f"{v2ray_link}\n")
                        print(f"âœ… å·²å°† {len(extracted_v2ray_links)} ä¸ªV2rayé“¾æ¥è¿½åŠ åˆ° {v2ray_output_file}")
                    else:
                        print(f"âš ï¸ æœªåœ¨ {temp_file} ä¸­æ‰¾åˆ°V2rayè®¢é˜…é“¾æ¥")
                    if delete_temp_files:
                        try:
                            os.remove(temp_file)
                            print(f"âœ… ä¸´æ—¶æ–‡ä»¶ {temp_file} å·²åˆ é™¤")
                        except Exception as e:
                            print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {temp_file} å¤±è´¥: {e}")
                else:
                    print(f"âŒ ä¿å­˜é¡µé¢å†…å®¹åˆ° {temp_file} å¤±è´¥")
            else:
                print(f"âŒ æ— æ³•è·å– {link} çš„é¡µé¢å†…å®¹")
        return True
    except Exception as e:
        print(f"âŒ å¤„ç†é“¾æ¥æ–‡ä»¶å¤±è´¥: {e}")
        return False

def main():
    print("ğŸš€ 85LAç½‘ç«™é“¾æ¥æå–å·¥å…·")
    print("=" * 60)
    
    url = "https://www.85la.com/"
    temp_file = "temp_page.html"
    links_file = "url.txt"
    v2ray_output_file = "links.txt"
    days = 3
    
    crawler = SimpleBrowserCrawler(headless=True)
    
    if not crawler.driver:
        print("âŒ æ— æ³•å¯åŠ¨æµè§ˆå™¨ï¼Œè¯·æ£€æŸ¥Chromeå’ŒChromeDriverå®‰è£…")
        return
    
    should_delete_temp_html = False  # è°ƒè¯•æ—¶Falseï¼Œå‘å¸ƒæ—¶True

    try:
        page_content = crawler.get_page_content(url)
        
        if page_content:
            if crawler.save_to_temp_file(page_content, temp_file):
                recent_links = extract_recent_links(temp_file, days)
                
                if recent_links:
                    save_links_to_file(recent_links, links_file)
                    process_links_from_file(links_file, crawler, should_delete_temp_html)
                else:
                    print(f"âš ï¸ æœªæ‰¾åˆ°æœ€è¿‘{days}å¤©å†…çš„é“¾æ¥")
                
                try:
                    os.remove(temp_file)
                    print(f"âœ… ä¸´æ—¶æ–‡ä»¶ {temp_file} å·²åˆ é™¤")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            else:
                print("âŒ ä¿å­˜é¡µé¢å†…å®¹åˆ°ä¸´æ—¶æ–‡ä»¶å¤±è´¥")
        else:
            print("âŒ æ— æ³•è·å–é¡µé¢å†…å®¹")
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
    finally:
        crawler.close()
        try:
            if os.path.exists(links_file):
                os.remove(links_file)
                print(f"âœ… ä¸´æ—¶æ–‡ä»¶ {links_file} å·²åˆ é™¤")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤ä¸´æ—¶æ–‡ä»¶ {links_file} å¤±è´¥: {e}")

    print("\n" + "=" * 60)
    print("é“¾æ¥æå–å’Œå¤„ç†å®Œæˆ")

if __name__ == "__main__":
    main()
